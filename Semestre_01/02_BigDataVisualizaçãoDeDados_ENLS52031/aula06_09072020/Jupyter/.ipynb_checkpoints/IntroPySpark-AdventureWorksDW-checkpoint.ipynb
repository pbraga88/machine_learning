{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Dataset AdventureWorksDW"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Operações SQL simples"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "A base **AdventureWorksDW** https://github.com/microsoft/sql-server-samples/ é bastante conhecida no mundo de dados Microsoft.\n\nVamos fazer algumas atividades com o SparkSQL, como desafios."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Carregando o PySpark"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# !pip install pyspark",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SQLContext",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "conf = SparkConf().setMaster('local').setAppName('PySpark SQL')\nsc = SparkContext.getOrCreate(conf = conf)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Criação do contexto do objeto SparkSQL que será responsável por executar as *query* do Spark com comandos SQL."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "sql = SQLContext(sc)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "sql",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Aqui é a criação de um Dataframe com os dados que estamos lendo do arquivo CSV. Usamos o contexto do SparkSQL, mas ainda sim é um Dataframe"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "FactInternetSales_Spark = sql.read.format(\"csv\").options(header='true').load('AdventureWorksDW/FactInternetSales.csv')\nDimSalesTerritory_Spark = sql.read.format(\"csv\").options(header='true').load('AdventureWorksDW/DimSalesTerritory.csv')\nDimProductSubcategory_Spark = sql.read.format(\"csv\").options(header='true').load('AdventureWorksDW/DimProductSubcategory.csv')\nDimProductCategory_Spark = sql.read.format(\"csv\").options(header='true').load('AdventureWorksDW/DimProductCategory.csv')\nDimProduct_Spark = sql.read.format(\"csv\").options(header='true').load('AdventureWorksDW/DimProduct.csv')\nDimCustomer_Spark = sql.read.format(\"csv\").options(header='true').load('AdventureWorksDW/DimCustomer.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "FactInternetSales_Spark.printSchema()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "FactInternetSales_Spark.select([\"ProductKey\",\"TotalProductCost\"]).show(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "A partir do dataframe **dadosSpark**, vamos registrar uma tabela temporária do SQL, chamada **Carros**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "FactInternetSales_Spark.registerTempTable(\"FactInternetSales\")\nDimSalesTerritory_Spark.registerTempTable(\"DimSalesTerritory\")\nDimProductSubcategory_Spark.registerTempTable(\"DimProductSubcategory\")\nDimProductCategory_Spark.registerTempTable(\"DimProductCategory\")\nDimProduct_Spark.registerTempTable(\"DimProduct\")\nDimCustomer_Spark.registerTempTable(\"DimCustomer\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "A partir do motor do **SparkSQL** vamos escrever uma *query* em SQL que retornará todas as linhas e colunas da nossa base"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "sql.sql(\"SELECT sub.* FROM \\\n            DimProductSubcategory as SUB inner join \\\n            DimProductCategory Cat \\\n                on SUB.ProductCategoryKey = Cat.ProductCategoryKey \\\n            where Cat.ProductCategoryKey = 2\").show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "sql.sql(\"\").show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "sql.sql(\"\").show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "sql.sql(\"\").show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "sql.sql(\"\").show()",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}